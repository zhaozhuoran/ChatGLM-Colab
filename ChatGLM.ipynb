{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/V/Xg2PgrUywlUv6cd2DQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaozhuoran/ChatGLM-Colab/blob/main/ChatGLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGLM-6B 介绍\n",
        "\n",
        "\n",
        "ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 [General Language Model (GLM)](https://github.com/THUDM/GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。\n",
        "ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。更多信息请参考我们的[博客](https://chatglm.cn/blog)。\n",
        "\n",
        "不过，由于ChatGLM-6B的规模较小，目前已知其具有相当多的[**局限性**](#局限性)，如事实性/数学逻辑错误，可能生成有害/有偏见内容，较弱的上下文能力，自我认知混乱，以及对英文指示生成与中文指示完全矛盾的内容。请大家在使用前了解这些问题，以免产生误解。\n",
        "\n",
        "源仓库地址 [https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)"
      ],
      "metadata": {
        "id": "a1IpyZobJ-OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 复制仓库"
      ],
      "metadata": {
        "id": "K2PvyGscKaUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmqSMXuRJUiC",
        "outputId": "d210e0da-67f3-44f5-b5e0-cc3081f0920f"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/THUDM/ChatGLM-6B\n",
        "%cd ChatGLM-6B"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安装依赖"
      ],
      "metadata": {
        "id": "rWhLYKUAKl5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "mkSr-K3oKoXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 启动webui"
      ],
      "metadata": {
        "id": "LU3Jt9r0KsoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gradio as gr\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).quantize(4).half().cuda()\n",
        "model = model.eval()\n",
        "\n",
        "MAX_TURNS = 20\n",
        "MAX_BOXES = MAX_TURNS * 2\n",
        "\n",
        "\n",
        "def predict(input, history=None):\n",
        "    if history is None:\n",
        "        history = []\n",
        "    response, history = model.chat(tokenizer, input, history)\n",
        "    updates = []\n",
        "    for query, response in history:\n",
        "        updates.append(gr.update(visible=True, value=\"用户：\" + query))\n",
        "        updates.append(gr.update(visible=True, value=\"ChatGLM-6B：\" + response))\n",
        "    if len(updates) < MAX_BOXES:\n",
        "        updates = updates + [gr.Textbox.update(visible=False)] * (MAX_BOXES - len(updates))\n",
        "    return [history] + updates\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    state = gr.State([])\n",
        "    text_boxes = []\n",
        "    for i in range(MAX_BOXES):\n",
        "        if i % 2 == 0:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"提问：\"))\n",
        "        else:\n",
        "            text_boxes.append(gr.Markdown(visible=False, label=\"回复：\"))\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
        "        with gr.Column(scale=1):\n",
        "            button = gr.Button(\"Generate\")\n",
        "    button.click(predict, [txt, state], [state] + text_boxes)\n",
        "demo.queue().launch(share=True, inbrowser=True)\n"
      ],
      "metadata": {
        "id": "_PbgO0HVNASS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
